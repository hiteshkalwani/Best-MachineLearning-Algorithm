{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster - Optimize solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Definition | Key |\n",
    "| --- | --- | --- |\n",
    "| survival | Survival |\t0 = No, 1 = Yes |\n",
    "| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Sex\t| |\n",
    "| Age | Age in years | |\n",
    "| sibsp | # of siblings / spouses aboard the Titanic | | \n",
    "| parch\t| # of parents / children aboard the Titanic | |\t\n",
    "| ticket | Ticket number | |\n",
    "| fare | Passenger fare | |\t\n",
    "| cabin | Cabin number | |\n",
    "| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe that some of the data are not that much meaningful while prediction, So we will remove some of the columns in preprocessing of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Read the data from given csv file\n",
    "df = pd.read_csv(\"./data/train.csv\") \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 6 columns):\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null object\n",
      "Age         714 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Survived    891 non-null int64\n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 41.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remove the non meaningful columns from the data\n",
    "df = df[['Pclass','Sex','Age','SibSp','Parch','Survived']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex  Survived  count\n",
       "0  female         0     81\n",
       "1  female         1    233\n",
       "2    male         0    468\n",
       "3    male         1    109"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the data with survival ratio\n",
    "sex_survived= df.groupby(by=['Sex','Survived'])['Survived'].agg(['count']).reset_index()\n",
    "sex_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass     Sex  Survived  count\n",
       "0        1  female         0      3\n",
       "1        1  female         1     91\n",
       "2        1    male         0     77\n",
       "3        1    male         1     45\n",
       "4        2  female         0      6\n",
       "5        2  female         1     70\n",
       "6        2    male         0     91\n",
       "7        2    male         1     17\n",
       "8        3  female         0     72\n",
       "9        3  female         1     72\n",
       "10       3    male         0    300\n",
       "11       3    male         1     47"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the data with survival ratio\n",
    "Pclass_survived= df.groupby(by=['Pclass','Sex','Survived'])['Pclass'].agg(['count']).reset_index()\n",
    "Pclass_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 6 columns):\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null object\n",
      "Age         891 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Survived    891 non-null int64\n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 41.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Interpolate the Age column data as the column has the less data compare to other columns\n",
    "df[['Age']] = df[['Age']].interpolate()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 6 columns):\n",
      "Pclass      891 non-null int64\n",
      "Sex         891 non-null int64\n",
      "Age         891 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Survived    891 non-null int64\n",
      "dtypes: float64(1), int64(5)\n",
      "memory usage: 41.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Simple encode the Sex column data, We can also use the Mapping \n",
    "number = LabelEncoder()\n",
    "df['Sex'] = number.fit_transform(df['Sex'].astype('str'))\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data \n",
    "X = df.copy()\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "X_train = X.iloc[:,:-1].values\n",
    "Y_train = X.iloc[:,5].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TPOT Genetic algorithm for dynamically finding the suitable pipeline for the problem \n",
    "tpot = TPOTClassifier(verbosity=3, \n",
    "                      scoring=\"balanced_accuracy\", \n",
    "                      random_state=23, \n",
    "                      periodic_checkpoint_folder=\"tpot_mnst1.txt\", \n",
    "                      n_jobs=-1, \n",
    "                      generations=10, \n",
    "                      population_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=1100, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.7935241919734402\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.7935241919734402\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_2_idx_0_2019.07.19_15-27-58.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_2_idx_1_2019.07.19_15-27-58.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7981815333789017\tLogisticRegression(ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_4_idx_0_2019.07.19_15-28-45.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_4_idx_1_2019.07.19_15-28-45.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_6_idx_2_2019.07.19_15-29-39.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7997233343098005\tGradientBoostingClassifier(MinMaxScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.05, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.8039758487126907\tRandomForestClassifier(CombineDFs(input_matrix, BernoulliNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_9_idx_1_2019.07.19_15-33-37.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_9_idx_2_2019.07.19_15-33-37.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.7975565944080982\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7997233343098005\tGradientBoostingClassifier(MinMaxScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.05, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.8039758487126907\tRandomForestClassifier(CombineDFs(input_matrix, BernoulliNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitesh/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=1100, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.7935241919734402\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.7935241919734402\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_2_idx_0_2019.07.19_15-34-45.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_2_idx_1_2019.07.19_15-34-45.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7981815333789017\tLogisticRegression(ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_4_idx_0_2019.07.19_15-35-30.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_4_idx_1_2019.07.19_15-35-30.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_6_idx_2_2019.07.19_15-36-24.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7997233343098005\tGradientBoostingClassifier(MinMaxScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.05, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.8039758487126907\tRandomForestClassifier(CombineDFs(input_matrix, BernoulliNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_9_idx_1_2019.07.19_15-40-26.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_9_idx_2_2019.07.19_15-40-26.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.7975565944080982\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7997233343098005\tGradientBoostingClassifier(MinMaxScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.05, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.8039758487126907\tRandomForestClassifier(CombineDFs(input_matrix, BernoulliNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitesh/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=1100, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.7935241919734402\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.7935241919734402\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_2_idx_0_2019.07.19_15-41-33.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_2_idx_1_2019.07.19_15-41-33.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7976241333854116\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7981815333789017\tLogisticRegression(ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_4_idx_0_2019.07.19_15-42-18.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_4_idx_1_2019.07.19_15-42-18.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_6_idx_2_2019.07.19_15-43-12.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7991763092796926\tRandomForestClassifier(Normalizer(input_matrix, Normalizer__norm=max), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8020013507795463\tLogisticRegression(PolynomialFeatures(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LogisticRegression__C=5.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.7956758780067051\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=2)\n",
      "-2\t0.7997233343098005\tGradientBoostingClassifier(MinMaxScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.05, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.8039758487126907\tRandomForestClassifier(CombineDFs(input_matrix, BernoulliNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_9_idx_1_2019.07.19_15-47-22.py\n",
      "Saving periodic pipeline from pareto front to tpot_mnst1.txt/pipeline_gen_9_idx_2_2019.07.19_15-47-22.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.7975565944080982\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.7997233343098005\tGradientBoostingClassifier(MinMaxScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.05, GradientBoostingClassifier__min_samples_leaf=18, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.8039758487126907\tRandomForestClassifier(CombineDFs(input_matrix, BernoulliNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=15, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hitesh/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables for comparision\n",
    "times = []\n",
    "winning_pipes = []\n",
    "scores = []\n",
    "\n",
    "# run three iterations and time them\n",
    "for x in range(3):\n",
    "    start_time = timeit.default_timer()\n",
    "    tpot.fit(X_train, y_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    times.append(elapsed)\n",
    "    winning_pipes.append(tpot.fitted_pipeline_)\n",
    "    scores.append(tpot.score(X_test, y_test))\n",
    "    tpot.export('tpot_titanic_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times: [6.766525390766644, 6.819817945249997, 6.930600284583306]\n",
      "Scores: [0.7904761904761904, 0.7904761904761904, 0.7904761904761904]\n",
      "Winning pipelines: [Pipeline(memory=None,\n",
      "         steps=[('featureunion',\n",
      "                 FeatureUnion(n_jobs=None,\n",
      "                              transformer_list=[('functiontransformer',\n",
      "                                                 FunctionTransformer(accept_sparse=False,\n",
      "                                                                     check_inverse=True,\n",
      "                                                                     func=<function copy at 0x7fa22bb95158>,\n",
      "                                                                     inv_kw_args=None,\n",
      "                                                                     inverse_func=None,\n",
      "                                                                     kw_args=None,\n",
      "                                                                     pass_y='deprecated',\n",
      "                                                                     validate=None)),\n",
      "                                                ('stackingestimator',\n",
      "                                                 StackingEstimator(estimator=Pipeline(mem...\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='entropy', max_depth=None,\n",
      "                                        max_features=0.1, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=6,\n",
      "                                        min_samples_split=18,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False), Pipeline(memory=None,\n",
      "         steps=[('featureunion',\n",
      "                 FeatureUnion(n_jobs=None,\n",
      "                              transformer_list=[('functiontransformer',\n",
      "                                                 FunctionTransformer(accept_sparse=False,\n",
      "                                                                     check_inverse=True,\n",
      "                                                                     func=<function copy at 0x7fa22bb95158>,\n",
      "                                                                     inv_kw_args=None,\n",
      "                                                                     inverse_func=None,\n",
      "                                                                     kw_args=None,\n",
      "                                                                     pass_y='deprecated',\n",
      "                                                                     validate=None)),\n",
      "                                                ('stackingestimator',\n",
      "                                                 StackingEstimator(estimator=Pipeline(mem...\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='entropy', max_depth=None,\n",
      "                                        max_features=0.1, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=6,\n",
      "                                        min_samples_split=18,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False), Pipeline(memory=None,\n",
      "         steps=[('featureunion',\n",
      "                 FeatureUnion(n_jobs=None,\n",
      "                              transformer_list=[('functiontransformer',\n",
      "                                                 FunctionTransformer(accept_sparse=False,\n",
      "                                                                     check_inverse=True,\n",
      "                                                                     func=<function copy at 0x7fa22bb95158>,\n",
      "                                                                     inv_kw_args=None,\n",
      "                                                                     inverse_func=None,\n",
      "                                                                     kw_args=None,\n",
      "                                                                     pass_y='deprecated',\n",
      "                                                                     validate=None)),\n",
      "                                                ('stackingestimator',\n",
      "                                                 StackingEstimator(estimator=Pipeline(mem...\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='entropy', max_depth=None,\n",
      "                                        max_features=0.1, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=6,\n",
      "                                        min_samples_split=18,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)]\n"
     ]
    }
   ],
   "source": [
    "times = [time/60 for time in times]\n",
    "print('Times:', times)\n",
    "print('Scores:', scores)   \n",
    "print('Winning pipelines:', winning_pipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can observe the best fitted machine learning pipeline for given dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this pipeline. Above code will generate the file with best pipeline implementation. We will use the pipeline directly in below section for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy,validate=False),\n",
    "        StackingEstimator(estimator=make_pipeline(\n",
    "            StackingEstimator(estimator=XGBClassifier(learning_rate=0.1, max_depth=9, min_child_weight=15, n_estimators=100, nthread=1, subsample=0.35000000000000003)),\n",
    "            BernoulliNB(alpha=0.001, fit_prior=False)\n",
    "        ))\n",
    "    ),\n",
    "    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.1, min_samples_leaf=6, min_samples_split=18, n_estimators=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8059701492537313"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline.fit(X_train, y_train)\n",
    "results = exported_pipeline.predict(X_test)\n",
    "exported_pipeline.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
